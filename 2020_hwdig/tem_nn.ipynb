{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------调用相关依赖包------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_curve \n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.special import erfinv\n",
    "from collections import OrderedDict\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb '\n",
    "          '({:.1f}% reduction),'\n",
    "          'time spend:{:2.2f} min'.format(end_mem,\n",
    "                                          100 * (start_mem - end_mem) / start_mem, (time.time() - starttime) / 60))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------ 数据预处理----------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, chunk=0, sep_tag=','):\n",
    "    if chunk == 0:\n",
    "        data = pd.read_csv(file_path, sep=sep_tag)\n",
    "    else:\n",
    "        data_iter = pd.read_csv(file_path, sep=sep_tag, chunksize=chunk)\n",
    "        data = pd.DataFrame()\n",
    "        for chunk in data_iter:\n",
    "            chunk = (chunk)\n",
    "            data = pd.concat([data, chunk])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data('train_data.csv', sep_tag='|', chunk=2000000)\n",
    "test_data_A = load_data('test_data_A.csv', sep_tag='|')\n",
    "test_data_B = load_data('test_data_B.csv', sep_tag='|')\n",
    "test_data_B['pt_d'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([train_data, test_data_A, test_data_B]).reset_index(drop=True)\n",
    "\n",
    "cat_cols = ['task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id', 'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id',\n",
    "            'tags',\n",
    "            'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
    "            'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
    "            'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
    "            'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
    "            'up_membership_grade', 'membership_life_duration', 'consume_purchase',\n",
    "            'communication_avgonline_30d', 'indu_name']\n",
    "\n",
    "\n",
    "def Label_enco(df, cols):\n",
    "    for f in tqdm.tqdm(cols):\n",
    "        map_dict = dict(zip(df[f].unique(), range(df[f].nunique())))\n",
    "        df[f] = df[f].map(map_dict).fillna(-1).astype(np.int32)\n",
    "    return df\n",
    "\n",
    "\n",
    "data_all = Label_enco(data_all, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_all.loc[data_all['pt_d'] < 8].reset_index(drop=True)\n",
    "test_data = data_all.loc[data_all['pt_d'] == 8].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------模型训练----------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTR_dataset(Dataset):\n",
    "    def __init__(self,df,cate_cols,temp_adv_id,temp_adv_prim_id,weight_T,is_train=False):\n",
    "        self.weight_T =weight_T\n",
    "        self.cate_cols = cate_cols\n",
    "        if is_train:\n",
    "            self.label = torch.tensor(df.label.values,dtype = torch.float32)\n",
    "            self.weight = torch.tensor(self.get_weight(self.label) , dtype = torch.float32)\n",
    "        else:\n",
    "            self.label = None\n",
    "        self.col_dict = {}\n",
    "        for col in cate_cols:\n",
    "            self.col_dict[col] = torch.tensor(df[col].values, dtype = torch.int64)\n",
    "        self.col_dict['temp_adv_id'] = torch.tensor(temp_adv_id,dtype=torch.int64)\n",
    "        self.col_dict['temp_adv_prim_id'] = torch.tensor(temp_adv_prim_id,dtype=torch.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.col_dict['task_id'])\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        output  = {}\n",
    "        if self.label is not None:\n",
    "            output= {'label': self.label[item],\n",
    "            'weight':self.weight[item],\n",
    "            }\n",
    "        for col in self.cate_cols:\n",
    "            output[col] =  self.col_dict[col][item]\n",
    "    \n",
    "        output['temp_adv_id'] = self.col_dict['temp_adv_id'][item]\n",
    "        output['temp_adv_prim_id'] = self.col_dict['temp_adv_prim_id'][item]\n",
    "    \n",
    "        return {key: value for key, value in output.items()}\n",
    "\n",
    "    def get_weight(self,label):\n",
    "        weight = []\n",
    "        for i in label:\n",
    "            if i==1:\n",
    "                weight.append(self.weight_T)\n",
    "            else:\n",
    "                weight.append(1)\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Mish activation loaded...\")\n",
    "    def forward(self,x):\n",
    "        x = x * (torch.tanh(F.softplus(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, inputs_dim, hidden_units, dropout_rate=0, use_bn=False,\n",
    "                 init_std=0.0001, dice_dim=3, seed=1024):\n",
    "        super(DNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.seed = seed\n",
    "        self.use_bn = use_bn\n",
    "        if len(hidden_units) == 0:\n",
    "            raise ValueError(\"hidden_units is empty!!\")\n",
    "        hidden_units = [inputs_dim] + list(hidden_units)\n",
    "        self.Glu = nn.ModuleList([nn.GLU() for i in range(len(hidden_units)-1)])\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden_units[i], 2*hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.ModuleList(\n",
    "                [nn.BatchNorm1d(hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "        self.activation_layers = nn.ModuleList([nn.ReLU() for i in range(len(hidden_units) - 1)])\n",
    "        #self.activation_layers = nn.ModuleList([Mish() for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        # self.activation_layers = nn.ModuleList(\n",
    "        #     [activation_layer(activation, hidden_units[i + 1], dice_dim) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        for name, tensor in self.linears.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.linears)):\n",
    "\n",
    "            fc = self.Glu[i](self.linears[i](deep_input))\n",
    "\n",
    "            if self.use_bn:\n",
    "                fc = self.bn[i](fc)\n",
    "\n",
    "            fc = self.activation_layers[i](fc)\n",
    "\n",
    "            fc = self.dropout(fc)\n",
    "            deep_input = fc\n",
    "        return deep_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class temo_model(nn.Module):\n",
    "    def __init__(self,dict_id_word,cate_cols,emb_dim,inputs_dim,hid_dim, dnn_hidden_units=(512, 128),dnn_dropout=0.3,use_bn=True):\n",
    "        super(temo_model, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.cate_cols = cate_cols\n",
    "        emb_dict = {}\n",
    "\n",
    "        for col in cate_cols:\n",
    "            num_word = dict_id_word[col]['lenth']\n",
    "            emb_dict[col] = nn.Embedding(num_word,emb_dim)\n",
    "\n",
    "\n",
    "        self.emb_dict = nn.ModuleDict(emb_dict)\n",
    "\n",
    "        self.rnn1 = nn.GRU(emb_dim,128,batch_first=True,bidirectional =True)\n",
    "        self.rnn2 = nn.GRU(emb_dim,128,batch_first=True,bidirectional =True)\n",
    "        self.dnn = DNN(inputs_dim, dnn_hidden_units, dnn_dropout, use_bn)\n",
    "        self.linear_out = nn.Linear(dnn_hidden_units[-1],1)\n",
    "    def forward(self,x):\n",
    "        ad_col = ['adv_id','task_id','creat_type_cd','adv_prim_id','dev_id',\n",
    "                                'inter_type_cd','spread_app_id','tags','app_first_class','app_second_class','his_app_size','his_on_shelf_time',\n",
    "                                                                 'app_score','indu_name']\n",
    "        user_col = ['age','city','city_rank','device_name','device_size',\n",
    "                                'career','gender','net_type','residence','emui_dev','list_time','device_price','up_life_duration','up_membership_grade',\n",
    "                                            'membership_life_duration','consume_purchase','communication_avgonline_30d']\n",
    "        ad_embs = []\n",
    "        user_embs = []\n",
    "        for i ,col in enumerate(ad_col):\n",
    "            ad_embs.append(self.emb_dict[col](x[col].to(device)).unsqueeze(1))\n",
    "        for i ,col in enumerate(user_col):\n",
    "            user_embs.append(self.emb_dict[col](x[col].to(device)).unsqueeze(1))\n",
    "        ad_embs = torch.cat(ad_embs,dim=1)\n",
    "        user_embs = torch.cat(user_embs,dim=1)\n",
    "        ad_embs = torch.sum(ad_embs,dim=1)\n",
    "        user_embs = torch.sum(user_embs,dim=1)\n",
    "        fm_out = torch.sum(ad_embs*user_embs,dim=1)\n",
    "        embeding = [self.emb_dict[col](x[col].to(device)) for col in self.cate_cols ]\n",
    "        embeding = torch.cat(embeding,dim=-1)\n",
    "        temp_adv_id =self.emb_dict['adv_id'](x['temp_adv_id'].to(device))\n",
    "        _,temp_adv_id= self.rnn1(temp_adv_id)\n",
    "        temp_adv_id = temp_adv_id.permute(1,0,2)\n",
    "        batch_size = temp_adv_id.size()[0]\n",
    "        temp_adv_id = temp_adv_id.contiguous().view(batch_size,-1)\n",
    "        temp_adv_prim_id =self.emb_dict['adv_prim_id'](x['temp_adv_prim_id'].to(device))\n",
    "        _,temp_adv_prim_id= self.rnn2(temp_adv_prim_id)\n",
    "        temp_adv_prim_id = temp_adv_prim_id.permute(1,0,2)\n",
    "        batch_size = temp_adv_prim_id.size()[0]\n",
    "        temp_adv_prim_id = temp_adv_prim_id.contiguous().view(batch_size,-1)\n",
    "        dnn_input = torch.cat((embeding,temp_adv_id,temp_adv_prim_id),dim=-1)\n",
    "        dnn_output  = self.linear_out(self.dnn(dnn_input)).squeeze(1)\n",
    "        logit = fm_out+dnn_output\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model,train_loader,train_shape,batch_size,optimizer,is_smooth,eps):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    auc = 0\n",
    "    avg_loss = 0\n",
    "    static = 0\n",
    "    pred = torch.zeros((train_shape)).to(device)\n",
    "    true_label = torch.zeros((train_shape)).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer.zero_grad()\n",
    "    tk0 = tqdm.tqdm(train_loader, total=len(train_loader))\n",
    "    for idx, batch in enumerate(tk0):\n",
    "\n",
    "\n",
    "        label = batch['label'].view(-1).to(device)\n",
    "\n",
    "        if is_smooth =='double':\n",
    "            label = (1-eps)*label+(1-label)*eps/1\n",
    "        elif is_smooth =='one':\n",
    "            label = label+(1-label)*eps/1\n",
    "        weight = batch['weight'].view(-1).to(device)\n",
    "        output_train = model(batch)\n",
    "        pred[idx*batch_size:(idx+1)*batch_size] = (output_train.view(-1))\n",
    "        true_label[idx*batch_size:(idx+1)*batch_size] = label.view(-1)\n",
    "        loss1 = criterion(output_train.view(-1),label)\n",
    "\n",
    "        loss = loss1\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        tk0.set_postfix(loss=avg_loss/(idx+1))\n",
    "    auc = sklearn.metrics.roc_auc_score( (true_label).detach().cpu().squeeze().numpy(),torch.sigmoid(pred).detach().cpu().squeeze().numpy())\n",
    "    plt.plot(train_loss)\n",
    "    return avg_loss/(idx+1-static),auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fn(model,valid_loader,val_shape,batch_size):\n",
    "    model.eval()\n",
    "    auc = 0\n",
    "    avg_loss = 0\n",
    "    static = 0\n",
    "    sum = 0\n",
    "    my_loss = nn.BCEWithLogitsLoss()\n",
    "    pred = np.zeros((val_shape))\n",
    "    predict = np.zeros((val_shape))\n",
    "    true_label = np.zeros((val_shape))\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm.tqdm(valid_loader, total=len(valid_loader))\n",
    "        for idx, batch in enumerate(tk0):\n",
    "\n",
    "            label = batch['label'].to(device).view(-1)\n",
    "            weight = batch['weight'].to(device).view(-1)\n",
    "            output_train= model(batch)\n",
    "            pred[idx*batch_size:(idx+1)*batch_size] = torch.sigmoid(output_train.view(-1)).detach().cpu().squeeze().numpy()\n",
    "            predict[idx*batch_size:(idx+1)*batch_size] = output_train.view(-1).detach().cpu().squeeze().numpy()\n",
    "            true_label[idx*batch_size:(idx+1)*batch_size] = label.view(-1).detach().cpu().squeeze().numpy()\n",
    "            loss = my_loss(output_train.view(-1),label)\n",
    "            avg_loss += loss.item()\n",
    "            tk0.set_postfix(loss=avg_loss/(idx+1))\n",
    "   \n",
    "        auc = sklearn.metrics.roc_auc_score( true_label,pred)\n",
    "    return avg_loss/(idx+1-static),auc,predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(model,test_loader,test_shape,batch_size):\n",
    "    model.eval()\n",
    "    pred = torch.zeros((test_shape))\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm_notebook(enumerate(test_loader),mininterval=2,desc='--testing',leave=False):\n",
    "            output_train= model(batch)\n",
    "            pred[idx*batch_size:(idx+1)*batch_size] = output_train.view(-1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_data, test_data, cols):    \n",
    "    temp_data = train_data[train_data['label']==1].groupby(['uid','pt_d']).agg({cols:list}).reset_index()\n",
    "    temp_data = temp_data.sort_values(['uid','pt_d'])\n",
    "    \n",
    "    uid = temp_data['uid'].tolist()\n",
    "    pt_d = temp_data['pt_d'].tolist()\n",
    "    adv_id = temp_data[cols].tolist()\n",
    "    \n",
    "    res = []\n",
    "    for i in tqdm.tqdm(range(0,len(uid))):\n",
    "        res.append([uid[i],pt_d[i],','.join(list(map(str,adv_id[i])))])\n",
    "    total_days = 8\n",
    "    result = [[res[0][0],res[0][1]+1,res[0][2]]]\n",
    "    for i in tqdm.tqdm(range(1,len(res))):\n",
    "        if res[i][0] == result[-1][0]:\n",
    "            while res[i][1]>result[-1][1]:\n",
    "                result.append([result[-1][0],result[-1][1]+1,result[-1][2]])\n",
    "            while res[i][1]>=result[-1][1]:\n",
    "                result.append([result[-1][0], result[-1][1] + 1, result[-1][2]+','+res[i][2]])\n",
    "        else:\n",
    "            while total_days>result[-1][1]:\n",
    "                result.append([result[-1][0],result[-1][1]+1,result[-1][2]])\n",
    "            result.append([res[i][0], res[i][1]+1, res[i][2]])\n",
    "            \n",
    "    result = pd.DataFrame(result)\n",
    "    result.columns = ['uid','pt_d','his_'+cols]\n",
    "    result['his_'+cols] = result['his_'+cols].apply(lambda x:list(map(int,x.split(','))))\n",
    "    result['his_'+cols+'_len'] =result['his_'+cols].apply(lambda x:len(x))\n",
    "    train_data = train_data.merge(result,on = ['uid','pt_d'],how='left')\n",
    "    test_data = test_data.merge(result,on = ['uid','pt_d'],how='left')\n",
    "    train_data = train_data.fillna(0)\n",
    "    test_data = test_data.fillna(0)\n",
    "    \n",
    "    his_adv_id = train_data['his_'+cols].tolist()\n",
    "    for i in tqdm.tqdm(range(len(his_adv_id))):\n",
    "        if his_adv_id[i]==0:\n",
    "            his_adv_id[i] = [0]*20\n",
    "        else:\n",
    "            his_adv_id[i] =[0]*(20-len(his_adv_id[i]))+his_adv_id[i][::-1][0:20]\n",
    "    his_adv_id = np.array(his_adv_id)\n",
    "    \n",
    "    test_his_adv_id = test_data['his_'+cols].tolist()\n",
    "    for i in tqdm.tqdm(range(len(test_his_adv_id))):\n",
    "        if test_his_adv_id[i]==0:\n",
    "            test_his_adv_id[i] = [0]*20\n",
    "        else:\n",
    "            test_his_adv_id[i] =[0]*(20-len(test_his_adv_id[i]))+test_his_adv_id[i][::-1][0:20]\n",
    "            \n",
    "    test_his_adv_id = np.array(test_his_adv_id)\n",
    "    return train_data, test_data,his_adv_id,test_his_adv_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data,train_his_advid,test_his_advid = process_data(train_data, test_data, 'adv_id')\n",
    "train_data,test_data,train_his_adv_prim_id,test_his_adv_prim_id = process_data(train_data, test_data, 'adv_prim_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floder = StratifiedKFold(n_splits=5,random_state=42,shuffle=True)\n",
    "predicts = np.zeros(len(train_data))\n",
    "Batch_size = 8192*2\n",
    "device = torch.device('cuda')\n",
    "NUM_EPOCH = 5\n",
    "test_predicts = np.zeros(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
    "       'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
    "       'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
    "       'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
    "       'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
    "       'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
    "       'up_membership_grade', 'membership_life_duration', 'consume_purchase',\n",
    "        'communication_avgonline_30d', 'indu_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "dict_id_word = {}\n",
    "for col in cat_cols:\n",
    "  par_col={}\n",
    "  par_col['lenth'] = len(set(df[col].values))\n",
    "  dict_id_word[col] = par_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for fold,(train, test) in enumerate(floder.split(train_data,train_data.label)):\n",
    "\n",
    "  #data_process \n",
    "\n",
    "  train_dataset = CTR_dataset(train_data.iloc[train].reset_index(drop=True),cat_cols,train_his_advid[train],train_his_adv_prim_id[train],1,True)\n",
    "  valid_dataset = CTR_dataset(train_data.iloc[test].reset_index(drop=True),cat_cols,train_his_advid[test],train_his_adv_prim_id[test],1,True)\n",
    "  train_dataloader = DataLoader(train_dataset , batch_size = Batch_size,num_workers=4, shuffle=True)\n",
    "  valid_dataloader = DataLoader(valid_dataset , batch_size = Batch_size,num_workers=4, shuffle=False)\n",
    " \n",
    "  model  = temo_model(dict_id_word,cat_cols,16,1024,128, dnn_hidden_units=(1024, 512),dnn_dropout=0.3,use_bn=True)\n",
    "  model.zero_grad();\n",
    "  model  = nn.DataParallel(model)\n",
    "  model.to(device)\n",
    "\n",
    "    #优化器\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=0.01,weight_decay=0.1)\n",
    "  \n",
    "  \n",
    "  best_score = -1\n",
    "  for epoch in range(5):\n",
    "        torch.cuda.empty_cache()\n",
    "        start_time = time.time()\n",
    "        train_loss, train_auc = train_fn(model  ,train_dataloader ,len(train_dataset), Batch_size,optimizer,'no',0.1)\n",
    "        valid_loss,val_auc,predict, = val_fn(model ,valid_dataloader ,len(valid_dataset), Batch_size)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('epoch{}/{} , train_loss: {}  ,train_auc: {} \\n \\n val_loss :{} ,  val_auc:{} ,time: {}\\n'.format(epoch+1,NUM_EPOCH,train_loss,train_auc,valid_loss,val_auc,elapsed_time))\n",
    "        if val_auc>best_score:\n",
    "          best_score = val_auc\n",
    "          best_param_score = model.state_dict()\n",
    "          #predicts[test] = predict\n",
    "          torch.save(best_param_score,'temp__best_param_score_{}'.format(fold+1))\n",
    "      \n",
    "      #test_predict = test_fn(model ,test_dataloader ,len(test_dataset), Batch_size)\n",
    "  #test_predicts+=test_predict\n",
    "  del train_dataset\n",
    "  del valid_dataset\n",
    "  del train_dataloader\n",
    "  del valid_dataloader\n",
    "  gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------预测代码-----------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CTR_dataset(test_data,cat_cols,test_his_advid,test_his_adv_prim_id,1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset , batch_size = Batch_size,num_workers=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model1  = temo_model(dict_id_word,cat_cols,16,1024,128, dnn_hidden_units=(1024, 512),dnn_dropout=0.3,use_bn=True)\n",
    "model1.zero_grad();\n",
    "model1  = nn.DataParallel(model1)\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load('temp__best_param_score_{}'.format(1)))\n",
    "\n",
    "model2  = temo_model(dict_id_word,cat_cols,16,1024,128, dnn_hidden_units=(1024, 512),dnn_dropout=0.3,use_bn=True)\n",
    "model2.zero_grad();\n",
    "model2  = nn.DataParallel(model2)\n",
    "model2.to(device)\n",
    "model2.load_state_dict(torch.load('temp__best_param_score_{}'.format(2)))\n",
    "\n",
    "model3  = temo_model(dict_id_word,cat_cols,16,1024,128, dnn_hidden_units=(1024, 512),dnn_dropout=0.3,use_bn=True)\n",
    "model3.zero_grad();\n",
    "model3  = nn.DataParallel(model3)\n",
    "model3.to(device)\n",
    "model3.load_state_dict(torch.load('temp__best_param_score_{}'.format(3)))\n",
    "\n",
    "model4  = temo_model(dict_id_word,cat_cols,16,1024,128, dnn_hidden_units=(1024, 512),dnn_dropout=0.3,use_bn=True)\n",
    "model4.zero_grad();\n",
    "model4  = nn.DataParallel(model4)\n",
    "model4.to(device)\n",
    "model4.load_state_dict(torch.load('temp__best_param_score_{}'.format(4)))\n",
    "\n",
    "model5  = temo_model(dict_id_word,cat_cols,16,1024,128, dnn_hidden_units=(1024, 512),dnn_dropout=0.3,use_bn=True)\n",
    "model5.zero_grad();\n",
    "model5  = nn.DataParallel(model5)\n",
    "model5.to(device)\n",
    "model5.load_state_dict(torch.load('temp__best_param_score_{}'.format(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict1 = test_fn(model1 ,test_dataloader ,len(test_dataset), Batch_size)\n",
    "print((test_predict1>0).sum())\n",
    "test_predict2 = test_fn(model2 ,test_dataloader ,len(test_dataset), Batch_size)\n",
    "print((test_predict2>0).sum())\n",
    "\n",
    "test_predict3 = test_fn(model3 ,test_dataloader ,len(test_dataset), Batch_size)\n",
    "test_predict4 = test_fn(model4 ,test_dataloader ,len(test_dataset), Batch_size)\n",
    "test_predict5 = test_fn(model5 ,test_dataloader ,len(test_dataset), Batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict1 = test_predict1[1000000:]\n",
    "test_predict2 = test_predict2[1000000:]\n",
    "test_predict3 = test_predict3[1000000:]\n",
    "test_predict4 = test_predict4[1000000:]\n",
    "test_predict5 = test_predict5[1000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = (test_predict1+test_predict2 +test_predict3 +test_predict4 +test_predict5)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_sig = torch.sigmoid(test_predict)\n",
    "test_predict_sig = test_predict_sig.detach().cpu().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_predict_sig>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_test = pd.DataFrame(test_predict_sig).reset_index()\n",
    "stack_test.columns=['id','probability']\n",
    "stack_test['id']+=1\n",
    "stack_test.to_csv('temp_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
