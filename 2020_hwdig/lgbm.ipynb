{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_curve \n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.special import erfinv\n",
    "from collections import OrderedDict\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder as Encoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders import CountEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lower_sample_data_by_sample(df,percent=1,rs=42):\n",
    "    most_data = df[df['label'] == 0]  # 多数类别的样本\n",
    "    minority_data = df[df['label'] == 1]  # 少数类别的样本   \n",
    "    #随机采样most_data中的数据\n",
    "    lower_data=most_data.sample(n=int(percent*len(minority_data)),replace=False,random_state=rs,axis=0)   \n",
    "    return (pd.concat([lower_data,minority_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_train(df,samp):\n",
    "  if random.random()<samp:\n",
    "    return -1\n",
    "  else :\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------数据预处理----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'uid', 'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
    "       'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
    "       'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
    "       'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
    "       'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
    "       'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
    "       'up_membership_grade', 'membership_life_duration', 'consume_purchase',\n",
    "       'communication_onlinerate', 'communication_avgonline_30d', 'indu_name',\n",
    "       'pt_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                读取数据集\n",
    "train_df = reduce_mem(pd.read_csv('train_data.csv',sep='|'))\n",
    "#train_df = train_df.drop_duplicates(subset=columns)\n",
    "\n",
    "test_df = pd.read_csv('test_data_B.csv',sep='|')\n",
    "test_df['pt_d']-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_A = pd.read_csv('test_data_A.csv',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无用列\n",
    "drop_cols = ['pt_d','label','communication_onlinerate','index','id','K']\n",
    "\n",
    "# 选择类别特征\n",
    "cat_cols = [ 'uid', 'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
    "       'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
    "       'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
    "       'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
    "       'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
    "       'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
    "       'up_membership_grade', 'membership_life_duration', 'consume_purchase'\n",
    "        , 'communication_avgonline_30d', 'indu_name',\n",
    "      ]\n",
    "MASK = 'MASK'\n",
    "miss_col1 = ['task_id', 'adv_id','uid']\n",
    "miss_col2 = ['adv_prim_id','dev_id' ]#, 'device_size','spread_app_id','indu_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_history_cols = rh_col =  [ 'uid', 'task_id', 'adv_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([test_df_A,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['id'] = 0\n",
    "train_df['first_online'] = train_df['communication_onlinerate'].apply(lambda x:int(x.split('^')[0]))\n",
    "train_df['last_online'] = train_df['communication_onlinerate'].apply(lambda x:int(x.split('^')[-1]))\n",
    "test_df['first_online'] = test_df['communication_onlinerate'].apply(lambda x:int(x.split('^')[0]))\n",
    "test_df['last_online'] = test_df['communication_onlinerate'].apply(lambda x:int(x.split('^')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(train,test,key1,key2):\n",
    "    \n",
    "    train_tif = pd.DataFrame(train[[key1, key2]].groupby([key1])[key2].apply(list))\n",
    "    train_tif.reset_index(inplace=True)\n",
    "    train_key1= train_tif[key1].values\n",
    "    train_key2    = train_tif[key2].values.tolist()\n",
    "    train_key2_list = []\n",
    "    for seq in train_key2:\n",
    "        sentences = []\n",
    "        for word in seq:\n",
    "            sentences.append(str(word))\n",
    "        train_key2_list.append(' '.join(sentences))\n",
    "    \n",
    "\n",
    "    tfidf_vec = TfidfVectorizer() \n",
    "    train_tfidf_matrix = tfidf_vec.fit_transform(train_key2_list).toarray()\n",
    "\n",
    "    test_tif = pd.DataFrame(test[[key1, key2]].groupby([key1])[key2].apply(list))\n",
    "    test_tif.reset_index(inplace=True)\n",
    "    test_key1= test_tif[key1].values\n",
    "    test_key2 = test_tif[key2].values.tolist()\n",
    "    test_key2_list = []\n",
    "    for seq in test_key2:\n",
    "        sentences = []\n",
    "        for word in seq:\n",
    "            sentences.append(str(word))\n",
    "        test_key2_list.append(' '.join(sentences))\n",
    "    test_tfidf_matrix = tfidf_vec.transform(test_key2_list).toarray()\n",
    "    assert train_tfidf_matrix.shape[1]==test_tfidf_matrix.shape[1]\n",
    "    \n",
    "    train_tfidf_agmax = np.argmax(train_tfidf_matrix,axis=1)\n",
    "    train_tfidf_max = np.max(train_tfidf_matrix,axis=1)\n",
    "    train_tfidf_mean = np.mean(train_tfidf_matrix,axis=1)\n",
    "    train_tfidf_std = np.std(train_tfidf_matrix,axis=1)\n",
    "    \n",
    "    test_tfidf_agmax = np.argmax(test_tfidf_matrix,axis=1)\n",
    "    test_tfidf_max = np.max(test_tfidf_matrix,axis=1)\n",
    "    test_tfidf_mean = np.mean(test_tfidf_matrix,axis=1)\n",
    "    test_tfidf_std = np.std(test_tfidf_matrix,axis=1)\n",
    "    \n",
    "    print('train_tfidf_agmax.shape:')\n",
    "    print(train_tfidf_agmax.shape)\n",
    "    \n",
    "    print('train_tfidf_mean.shape:')\n",
    "    print(train_tfidf_mean.shape)\n",
    "    \n",
    "    print('test_tfidf_agmax.shape:')\n",
    "    print(test_tfidf_agmax.shape)\n",
    "    \n",
    "    print('test_tfidf_mean.shape:')\n",
    "    print(test_tfidf_mean.shape)\n",
    "    return train_tif,test_tif,train_tfidf_agmax,train_tfidf_max,train_tfidf_mean,train_tfidf_std,test_tfidf_agmax,test_tfidf_max,test_tfidf_mean,test_tfidf_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temporal_feature(train,test,col):\n",
    "    \n",
    "    print('统计昨天点击数')\n",
    "    \n",
    "    label_sum = train.groupby([col,'pt_d'])['label'].agg('sum').reset_index()\n",
    "    label_sum.columns = [col,'pt_d',col+'yesterday_label_sum']\n",
    "    label_sum['pt_d']+=1\n",
    "    test = test.merge(label_sum,on = [col,'pt_d'],how = 'left')\n",
    "    train = train.merge(label_sum,on = [col,'pt_d'],how = 'left')\n",
    "    \n",
    "    print('统计今天曝光数')\n",
    "    \n",
    "    today_count_train = train.groupby([col,'pt_d'])['id'].agg('count').reset_index()\n",
    "    today_count_train.columns = [col,'pt_d',col+'today_expos_count']\n",
    "    train = train.merge(today_count_train,on = [col,'pt_d'],how = 'left')\n",
    "\n",
    "    today_count_test = test.groupby([col,'pt_d'])['id'].agg('count').reset_index()\n",
    "    today_count_test.columns = [col,'pt_d',col+'today_expos_count']\n",
    "    test = test.merge(today_count_test,on = [col,'pt_d'],how = 'left')\n",
    "    test[col+'today_expos_count']*=3\n",
    "    \n",
    "    print('统计昨天曝光数')\n",
    "    \n",
    "    expos = train.groupby([col,'pt_d'])['id'].agg('count').reset_index()\n",
    "    expos.columns = [col,'pt_d',col+'yesterday_expos_count']\n",
    "    expos['pt_d']+=1\n",
    "    test = test.merge(expos,on = [col,'pt_d'],how = 'left')\n",
    "    train = train.merge(expos,on = [col,'pt_d'],how = 'left')\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in count_history_cols:\n",
    "    train_df,test_df = get_temporal_feature(train_df,test_df,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[train_df.pt_d!=1].reset_index(drop=True)\n",
    "test_df = test_df.iloc[1000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_tif_uid1,test_tif_uid1,train_tfidf_agmax,train_tfidf_max,train_tfidf_mean,train_tfidf_std,test_tfidf_agmax,test_tfidf_max,test_tfidf_mean,test_tfidf_std = get_tfidf(train_df , test_df, 'uid','task_id')\n",
    "# train_tif_uid1 = train_tif_uid1.drop('task_id',axis=1)\n",
    "# train_tif_uid1['uid'+'task_id'+'tf_argmax'] = train_tfidf_agmax\n",
    "# train_tif_uid1['uid'+'task_id'+'max'] = train_tfidf_max\n",
    "# train_tif_uid1['uid'+'task_id'+'mean'] = train_tfidf_mean\n",
    "# train_tif_uid1['uid'+'task_id'+'std'] = train_tfidf_std\n",
    "\n",
    "# train_tif_uid2,test_tif,train_tfidf_agmax,train_tfidf_max,train_tfidf_mean,train_tfidf_std,test_tfidf_agmax,test_tfidf_max,test_tfidf_mean,test_tfidf_std = get_tfidf(train_df , test_df, 'uid','adv_id')\n",
    "# train_tif_uid2 = train_tif_uid2.drop('adv_id',axis=1)\n",
    "# train_tif_uid2['uid'+'adv_id'+'tf_argmax'] = train_tfidf_agmax\n",
    "# train_tif_uid2['uid'+'adv_id'+'max'] = train_tfidf_max\n",
    "# train_tif_uid2['uid'+'adv_id'+'mean'] = train_tfidf_mean\n",
    "# train_tif_uid2['uid'+'adv_id'+'std'] = train_tfidf_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = [i for i in train_df.columns if i not in drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------模型训练----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame()\n",
    "predicts = np.zeros(len(train_df))\n",
    "pred = np.zeros(len(test_df))\n",
    "true = np.zeros(len(train_df))\n",
    "params = {'learning_rate': 0.1,\n",
    "         'boosting': 'gbdt',\n",
    "         'objective':'binary',\n",
    "         'boosting': 'gbdt',\n",
    "         'num_leaves': 60,\n",
    "         'metric': 'auc',\n",
    "         'lambda_l1': 0.1,\n",
    "         'random_state': 42,\n",
    "         'verbosity': -1,\n",
    "         'num_threads' : -1,\n",
    "           \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.8 ,\n",
    "\n",
    "         }\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds.split(train_df, train_df.label)):\n",
    "    \n",
    "    train = train_df.iloc[train_idx][feature]\n",
    "    t_label = train_df.iloc[train_idx].label\n",
    "    valid = train_df.iloc[val_idx][feature]\n",
    "    v_label = train_df.iloc[val_idx].label\n",
    "    \n",
    "    train_dataset = lgb.Dataset(train,label=t_label,categorical_feature=cat_cols)\n",
    "    valid_dataset = lgb.Dataset(valid,v_label,categorical_feature=cat_cols)\n",
    "\n",
    "    num_round = 30000\n",
    "    clf = lgb.train(params,train_dataset,num_round,valid_sets=[train_dataset, valid_dataset],verbose_eval=50,early_stopping_rounds=100)\n",
    "    predicts[val_idx] = clf.predict(valid,num_iteration=clf.best_iteration)\n",
    "    pred += clf.predict(test_df[feature], num_iteration=clf.best_iteration)\n",
    "print(sklearn.metrics.roc_auc_score(train_df.label,predicts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------模型预测----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8092833835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred>1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res['id'] = test_df['id'].astype('int32')\n",
    "res['probability'] = pred\n",
    "res.to_csv('lgbm.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
