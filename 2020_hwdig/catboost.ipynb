{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.metrics import roc_curve \n",
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.special import erfinv\n",
    "from collections import OrderedDict\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import catboost as cbt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder as Encoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders import CountEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lower_sample_data_by_sample(df,percent=1,rs=42):\n",
    "    most_data = df[df['label'] == 0]  # 多数类别的样本\n",
    "    minority_data = df[df['label'] == 1]  # 少数类别的样本   \n",
    "    #随机采样most_data中的数据\n",
    "    lower_data=most_data.sample(n=int(percent*len(minority_data)),replace=False,random_state=rs,axis=0)   \n",
    "    return (pd.concat([lower_data,minority_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_train(df,samp):\n",
    "  if random.random()<samp:\n",
    "    return -1\n",
    "  else :\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------数据预处理--------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'uid', 'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
    "       'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
    "       'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
    "       'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
    "       'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
    "       'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
    "       'up_membership_grade', 'membership_life_duration', 'consume_purchase',\n",
    "       'communication_onlinerate', 'communication_avgonline_30d', 'indu_name',\n",
    "       'pt_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                读取数据集\n",
    "train_df = reduce_mem(pd.read_csv('train_data.csv',sep='|'))\n",
    "\n",
    "test_df = pd.read_csv('test_data_B.csv',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(train,test,key1,key2):\n",
    "    \n",
    "    train_tif = pd.DataFrame(train[[key1, key2]].groupby([key1])[key2].apply(list))\n",
    "    train_tif.reset_index(inplace=True)\n",
    "    train_key1= train_tif[key1].values\n",
    "    train_key2    = train_tif[key2].values.tolist()\n",
    "    train_key2_list = []\n",
    "    for seq in train_key2:\n",
    "        sentences = []\n",
    "        for word in seq:\n",
    "            sentences.append(str(word))\n",
    "        train_key2_list.append(' '.join(sentences))\n",
    "    \n",
    "\n",
    "    tfidf_vec = TfidfVectorizer() \n",
    "    train_tfidf_matrix = tfidf_vec.fit_transform(train_key2_list).toarray()\n",
    "\n",
    "    test_tif = pd.DataFrame(test[[key1, key2]].groupby([key1])[key2].apply(list))\n",
    "    test_tif.reset_index(inplace=True)\n",
    "    test_key1= test_tif[key1].values\n",
    "    test_key2 = test_tif[key2].values.tolist()\n",
    "    test_key2_list = []\n",
    "    for seq in test_key2:\n",
    "        sentences = []\n",
    "        for word in seq:\n",
    "            sentences.append(str(word))\n",
    "        test_key2_list.append(' '.join(sentences))\n",
    "    test_tfidf_matrix = tfidf_vec.transform(test_key2_list).toarray()\n",
    "    assert train_tfidf_matrix.shape[1]==test_tfidf_matrix.shape[1]\n",
    "    \n",
    "    train_tfidf_agmax = np.argmax(train_tfidf_matrix,axis=1)\n",
    "    train_tfidf_max = np.max(train_tfidf_matrix,axis=1)\n",
    "    train_tfidf_mean = np.mean(train_tfidf_matrix,axis=1)\n",
    "    train_tfidf_std = np.std(train_tfidf_matrix,axis=1)\n",
    "    \n",
    "    test_tfidf_agmax = np.argmax(test_tfidf_matrix,axis=1)\n",
    "    test_tfidf_max = np.max(test_tfidf_matrix,axis=1)\n",
    "    test_tfidf_mean = np.mean(test_tfidf_matrix,axis=1)\n",
    "    test_tfidf_std = np.std(test_tfidf_matrix,axis=1)\n",
    "    \n",
    "    print('train_tfidf_agmax.shape:')\n",
    "    print(train_tfidf_agmax.shape)\n",
    "    \n",
    "    print('train_tfidf_mean.shape:')\n",
    "    print(train_tfidf_mean.shape)\n",
    "    \n",
    "    print('test_tfidf_agmax.shape:')\n",
    "    print(test_tfidf_agmax.shape)\n",
    "    \n",
    "    print('test_tfidf_mean.shape:')\n",
    "    print(test_tfidf_mean.shape)\n",
    "    return train_tif,test_tif,train_tfidf_agmax,train_tfidf_max,train_tfidf_mean,train_tfidf_std,test_tfidf_agmax,test_tfidf_max,test_tfidf_mean,test_tfidf_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 无用列\n",
    "drop_cols = ['pt_d','label','communication_onlinerate','index','id','K']\n",
    "\n",
    "# 选择类别特征\n",
    "cat_cols = [ 'uid', 'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
    "       'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
    "       'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
    "       'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
    "       'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
    "       'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
    "       'up_membership_grade', 'membership_life_duration', 'consume_purchase'\n",
    "        , 'communication_avgonline_30d', 'indu_name',\n",
    "      ]\n",
    "MASK = 'MASK'\n",
    "miss_col1 = ['task_id', 'adv_id','uid']\n",
    "miss_col2 = ['adv_prim_id','dev_id' ]#, 'device_size','spread_app_id','indu_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm_notebook( miss_col1):\n",
    "  train_df[col] = train_df[col].apply(lambda x :get_mask_train(x,0.1))\n",
    "for col in tqdm_notebook(miss_col2):\n",
    "  train_df[col] = train_df[col].apply(lambda x :get_mask_train(x,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm_notebook( miss_col1):\n",
    "  mask_list = list(set(test_df[col].values)-set(train_df[col].values))\n",
    "  print(len(mask_list)/len(set(test_df[col].values)))\n",
    "  test_df[col] = test_df[col].replace(mask_list,-1)\n",
    "for col in tqdm_notebook(miss_col2):\n",
    "  mask_list = list(set(test_df[col].values)-set(train_df[col].values))\n",
    "  print(len(mask_list)/len(set(test_df[col].values)))\n",
    "\n",
    "  test_df[col] = test_df[col].replace(mask_list,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_col = ['uid','age','city','city_rank','career','gender','residence','communication_avgonline_30d','consume_purchase','membership_life_duration','up_membership_grade','up_life_duration']\n",
    "ad_col = ['task_id','adv_id','creat_type_cd','adv_prim_id','dev_id','slot_id','spread_app_id','tags','app_first_class','app_second_class','indu_name','inter_type_cd']\n",
    "phone_col = ['device_name','device_size','net_type','emui_dev','device_price']\n",
    "app_col = ['his_app_size','his_on_shelf_time','app_score','list_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_tif_uid1,test_tif_uid1,train_tfidf_agmax,train_tfidf_max,train_tfidf_mean,train_tfidf_std,test_tfidf_agmax,test_tfidf_max,test_tfidf_mean,test_tfidf_std = get_tfidf(train_df , test_df, 'uid','task_id')\n",
    "train_tif_uid1 = train_tif_uid1.drop('task_id',axis=1)\n",
    "train_tif_uid1['uid'+'task_id'+'tf_argmax'] = train_tfidf_agmax\n",
    "train_tif_uid1['uid'+'task_id'+'max'] = train_tfidf_max\n",
    "train_tif_uid1['uid'+'task_id'+'mean'] = train_tfidf_mean\n",
    "train_tif_uid1['uid'+'task_id'+'std'] = train_tfidf_std\n",
    "\n",
    "train_tif_uid2,test_tif,train_tfidf_agmax,train_tfidf_max,train_tfidf_mean,train_tfidf_std,test_tfidf_agmax,test_tfidf_max,test_tfidf_mean,test_tfidf_std = get_tfidf(train_df , test_df, 'uid','adv_id')\n",
    "train_tif_uid2 = train_tif_uid2.drop('adv_id',axis=1)\n",
    "train_tif_uid2['uid'+'adv_id'+'tf_argmax'] = train_tfidf_agmax\n",
    "train_tif_uid2['uid'+'adv_id'+'max'] = train_tfidf_max\n",
    "train_tif_uid2['uid'+'adv_id'+'mean'] = train_tfidf_mean\n",
    "train_tif_uid2['uid'+'adv_id'+'std'] = train_tfidf_std\n",
    "\n",
    "train_tif_uid3,test_tif,train_tfidf_agmax,train_tfidf_max,train_tfidf_mean,train_tfidf_std,test_tfidf_agmax,test_tfidf_max,test_tfidf_mean,test_tfidf_std = get_tfidf(train_df , test_df, 'uid','slot_id')\n",
    "train_tif_uid3 = train_tif_uid3.drop('slot_id',axis=1)\n",
    "train_tif_uid3['uid'+'slot_id'+'tf_argmax'] = train_tfidf_agmax\n",
    "train_tif_uid3['uid'+'slot_id'+'max'] = train_tfidf_max\n",
    "train_tif_uid3['uid'+'slot_id'+'mean'] = train_tfidf_mean\n",
    "train_tif_uid3['uid'+'slot_id'+'std'] = train_tfidf_std\n",
    "\n",
    "train_tif_uid4,test_tif,train_tfidf_agmax,train_tfidf_max,train_tfidf_mean,train_tfidf_std,test_tfidf_agmax,test_tfidf_max,test_tfidf_mean,test_tfidf_std = get_tfidf(train_df , test_df, 'uid','adv_prim_id')\n",
    "train_tif_uid4 = train_tif_uid4.drop('adv_prim_id',axis=1)\n",
    "train_tif_uid4['uid'+'adv_prim_id'+'tf_argmax'] = train_tfidf_agmax\n",
    "train_tif_uid4['uid'+'adv_prim_id'+'max'] = train_tfidf_max\n",
    "train_tif_uid4['uid'+'adv_prim_id'+'mean'] = train_tfidf_mean\n",
    "train_tif_uid4['uid'+'adv_prim_id'+'std'] = train_tfidf_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series1=None, \n",
    "                  tst_series2 = None,\n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series1.name\n",
    "    assert trn_series.name == tst_series2.name\n",
    "    nui = trn_series.nunique()\n",
    "    cou = len(trn_series)\n",
    "    min_samples_leaf = min_samples_leaf*(cou/nui)\n",
    "    print(min_samples_leaf)\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    ft_tst_series1 = pd.merge(\n",
    "        tst_series1.to_frame(tst_series1.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series1.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    \n",
    "    ft_tst_series2 = pd.merge(\n",
    "        tst_series2.to_frame(tst_series2.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series2.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series1.index = tst_series1.index\n",
    "    ft_tst_series2.index = tst_series2.index\n",
    "    return add_noise(ft_trn_series, noise_level).values, add_noise(ft_tst_series1, noise_level).values,add_noise(ft_tst_series2, noise_level).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floder = StratifiedKFold(n_splits=5,random_state=42,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2 = test_df.copy()\n",
    "test_df3 = test_df.copy()\n",
    "test_df4 = test_df.copy()\n",
    "test_df5 = test_df.copy()\n",
    "test_df_list = [test_df , test_df2, test_df3, test_df4, test_df5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm_notebook(cat_cols):\n",
    "  i = 1\n",
    "  train_df[col + 'tar_enco'] = 0\n",
    "  train_df['K'] = 0\n",
    "  for k ,(tr_idx, oof_idx) in enumerate(StratifiedKFold(n_splits=5, random_state=2020, shuffle=True).split(train_df, train_df['label'])):\n",
    "    print('fold{}'.format(i))\n",
    "    i+=1\n",
    "    trn_series = train_df.iloc[tr_idx][col]\n",
    "    tst_series1 = train_df.iloc[oof_idx][col]\n",
    "    tst_series2 = test_df_list[k][col]\n",
    "    target = train_df.iloc[tr_idx].label\n",
    "    train_targetencoding,oof_targetencoding,test_targetencoding =  target_encode(trn_series, \n",
    "                  tst_series1, \n",
    "                  tst_series2,\n",
    "                  target, \n",
    "                  min_samples_leaf=0.2, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0.0001)\n",
    "    train_df.loc[oof_idx,col + 'tar_enco'] = oof_targetencoding\n",
    "    train_df.loc[oof_idx,'K'] = k\n",
    "    test_df_list[k][col + 'tar_enco'] = test_targetencoding\n",
    "  train_df = reduce_mem(train_df)\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = reduce_mem(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = lower_sample_data_by_sample(train_df , 3,303).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(train_tif_uid1,on='uid',how='left')\n",
    "train_df = train_df.merge(train_tif_uid2,on='uid',how='left')\n",
    "train_df = train_df.merge(train_tif_uid3,on='uid',how='left')\n",
    "train_df = train_df.merge(train_tif_uid4,on='uid',how='left')\n",
    "# train_df = train_df.merge(train_tif_taskid1,on='uid',how='left')\n",
    "# train_df = train_df.merge(train_tif_taskid2,on='uid',how='left')\n",
    "# train_df = train_df.merge(train_tif_advid1,on='uid',how='left')\n",
    "# train_df = train_df.merge(train_tif_advid1,on='uid',how='left')\n",
    "\n",
    "test_df = test_df.merge(train_tif_uid1,on='uid',how='left')\n",
    "test_df = test_df.merge(train_tif_uid2,on='uid',how='left')\n",
    "test_df = test_df.merge(train_tif_uid3,on='uid',how='left')\n",
    "test_df = test_df.merge(train_tif_uid4,on='uid',how='left')\n",
    "# test_df = test_df.merge(train_tif_taskid1,on='uid',how='left')\n",
    "# test_df = test_df.merge(train_tif_taskid2,on='uid',how='left')\n",
    "# test_df = test_df.merge(train_tif_advid1,on='uid',how='left')\n",
    "# test_df = test_df.merge(train_tif_advid1,on='uid',how='left')\n",
    "for i in range(5):\n",
    "    test_df_list[i] = test_df_list[i].merge(train_tif_uid1,on='uid',how='left')\n",
    "    test_df_list[i] = test_df_list[i].merge(train_tif_uid2,on='uid',how='left')\n",
    "    test_df_list[i] = test_df_list[i].merge(train_tif_uid3,on='uid',how='left')\n",
    "    test_df_list[i] = test_df_list[i].merge(train_tif_uid4,on='uid',how='left')\n",
    "#     test_df_list[i] = test_df_list[i].merge(train_tif_taskid1,on='uid',how='left')\n",
    "#     test_df_list[i] = test_df_list[i].merge(train_tif_taskid2,on='uid',how='left')\n",
    "#     test_df_list[i] = test_df_list[i].merge(train_tif_advid1,on='uid',how='left')\n",
    "#     test_df_list[i] = test_df_list[i].merge(train_tif_advid2,on='uid',how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = CountEncoder()\n",
    "for col in tqdm_notebook(cat_cols):\n",
    "      cl = CountEncoder(cols=col)\n",
    "      cl.fit(train_df[col])\n",
    "      train_df[col + '_count']  = (cl.transform(train_df[col])).values\n",
    "      test_df_list[0] = test_df_list[0].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "      test_df_list[1] = test_df_list[1].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "      test_df_list[2] = test_df_list[2].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "      test_df_list[3] = test_df_list[3].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "      test_df_list[4] = test_df_list[4].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = cat_cols+['user_kme','ad_kme','uidtask_idtf_argmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_feature = [col for col in train_df.columns if col not in drop_cols+cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = reduce_mem(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    test_df_list[i].drop(['communication_onlinerate'],axis=1,inplace=True)\n",
    "    test_df_list[i].fillna(0,inplace=True)\n",
    "    test_df_list[i]['K'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = cat_cols+dense_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_ad= KMeans(n_clusters=500, random_state=42)\n",
    "estimator_user= KMeans(n_clusters=500, random_state=42)\n",
    "\n",
    "user_col = ['age','city','city_rank','career','gender','residence']\n",
    "ad_col = ['task_id','adv_id','creat_type_cd','adv_prim_id','dev_id','slot_id','spread_app_id','tags','app_first_class','app_second_class','indu_name','inter_type_cd']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取Model\n",
    "with open('estimator.pickle', 'rb') as f:\n",
    "    estimator_user = pickle.load(f)\n",
    "    #测试读取后的Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取Model\n",
    "with open('estimator_ad.pickle', 'rb') as f:\n",
    "    estimator_ad = pickle.load(f)\n",
    "    #测试读取后的Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_features = []\n",
    "for col in train_df.columns:\n",
    "    for c  in ad_col:\n",
    "        if c+'tar_enco' in col:\n",
    "            ad_features.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = []\n",
    "for col in train_df.columns:\n",
    "    for c  in user_col:\n",
    "        if c+'tar_enco' in col:\n",
    "            user_features.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_pred =estimator_ad.predict(train_df[ad_features])\n",
    "train_df['ad_kme'] = ad_pred\n",
    "for i,t in enumerate(test_df_list):\n",
    "    test_df_list[i]['ad_kme'] = estimator_ad.predict(t[ad_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pred =estimator_user.predict(train_df[user_features])\n",
    "train_df['user_kme'] = user_pred\n",
    "for i,t in enumerate(test_df_list):\n",
    "    test_df_list[i]['user_kme'] = estimator_user.predict(t[user_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('estimator_ad.pickle', 'wb') as f:\n",
    "    pickle.dump(estimator_ad, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['user_kme'] = estimator_user.predict(test_df[user_features])\n",
    "test_df['ad_kme'] = estimator_ad.predict(test_df[ad_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm_notebook(['user_kme','ad_kme','uidtask_idtf_argmax','uidadv_idtf_argmax','uidslot_idtf_argmax','uidadv_prim_idtf_argmax']):\n",
    "      cl = CountEncoder(cols=col)\n",
    "      cl.fit(train_df[col])\n",
    "      train_df[col + '_count']  = (cl.transform(train_df[col])).values\n",
    "      test_df_list[0] = test_df_list[0].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "      test_df_list[1] = test_df_list[1].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "      test_df_list[2] = test_df_list[2].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "      test_df_list[3] = test_df_list[3].join(cl.transform(test_df[col]).add_suffix('_count'))\n",
    "      test_df_list[4] = test_df_list[4].join(cl.transform(test_df[col]).add_suffix('_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm_notebook(['user_kme','ad_kme','uidtask_idtf_argmax','uidadv_idtf_argmax','uidslot_idtf_argmax','uidadv_prim_idtf_argmax']):\n",
    "  i = 1\n",
    "  train_df[col + 'tar_enco'] = 0\n",
    "  train_df['K'] = 0\n",
    "  for k ,(tr_idx, oof_idx) in enumerate(StratifiedKFold(n_splits=5, random_state=2020, shuffle=True).split(train_df, train_df['label'])):\n",
    "    print('fold{}'.format(i))\n",
    "    i+=1\n",
    "    trn_series = train_df.iloc[tr_idx][col]\n",
    "    tst_series1 = train_df.iloc[oof_idx][col]\n",
    "    tst_series2 = test_df_list[k][col]\n",
    "    target = train_df.iloc[tr_idx].label\n",
    "    train_targetencoding,oof_targetencoding,test_targetencoding =  target_encode(trn_series, \n",
    "                  tst_series1, \n",
    "                  tst_series2,\n",
    "                  target, \n",
    "                  min_samples_leaf=0.2, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0.0001)\n",
    "    train_df.loc[oof_idx,col + 'tar_enco'] = oof_targetencoding\n",
    "    train_df.loc[oof_idx,'K'] = k\n",
    "    test_df_list[k][col + 'tar_enco'] = test_targetencoding\n",
    "  train_df = reduce_mem(train_df)\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1080\n",
    "is_shuffle=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_col = ['uid','age','city','city_rank','career','gender','residence','communication_avgonline_30d','consume_purchase','membership_life_duration','up_membership_grade','up_life_duration']\n",
    "ad_col = ['task_id','adv_id','creat_type_cd','adv_prim_id','dev_id','slot_id','spread_app_id','tags','app_first_class','app_second_class','indu_name','inter_type_cd']\n",
    "phone_col = ['device_name','device_size','net_type','emui_dev','device_price']\n",
    "app_col = ['his_app_size','his_on_shelf_time','app_score','list_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------模型训练----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for k in tqdm_notebook(range(5)):\n",
    "    t = train_df[train_df.K!=k].reset_index(drop=True)[user_col]\n",
    "    t_label = train_df[train_df.K!=k].reset_index(drop=True).label.values\n",
    "    v = train_df[train_df.K==k].reset_index(drop=True)[user_col]\n",
    "    v_label = train_df[train_df.K==k].reset_index(drop=True).label.values\n",
    "    \n",
    "    RF_user = RandomForestClassifier(n_estimators=10, criterion='gini',n_jobs=-1, random_state=42, verbose=1)\n",
    "    RF_user.fit(t,t_label)\n",
    "    train_df.loc[train_df[train_df.K==k].index,'rf_user'] = RF_user.predict_proba(v)[:,1]\n",
    "    test_df_list[k]['rf_user'] = RF_user.predict_proba(test_df_list[k][user_col])[:,1]\n",
    "\n",
    "for k in tqdm_notebook(range(5)):\n",
    "    t = train_df[train_df.K!=k].reset_index(drop=True)[ad_col]\n",
    "    t_label = train_df[train_df.K!=k].reset_index(drop=True).label.values\n",
    "    v = train_df[train_df.K==k].reset_index(drop=True)[ad_col]\n",
    "    v_label = train_df[train_df.K==k].reset_index(drop=True).label.values\n",
    "    \n",
    "    RF_ad = RandomForestClassifier(n_estimators=10, criterion='gini',n_jobs=-1, random_state=42, verbose=1)\n",
    "    RF_ad.fit(t,t_label)\n",
    "    train_df.loc[train_df[train_df.K==k].index,'rf_ad'] = RF_ad.predict_proba(v)[:,1]\n",
    "    test_df_list[k]['rf_ad'] = RF_ad.predict_proba(test_df_list[k][ad_col])[:,1]\n",
    "\n",
    "for k in tqdm_notebook(range(5)):\n",
    "    t = train_df[train_df.K!=k].reset_index(drop=True)[phone_col]\n",
    "    t_label = train_df[train_df.K!=k].reset_index(drop=True).label.values\n",
    "    v = train_df[train_df.K==k].reset_index(drop=True)[phone_col]\n",
    "    v_label = train_df[train_df.K==k].reset_index(drop=True).label.values\n",
    "    \n",
    "    RF_phone = RandomForestClassifier(n_estimators=10, criterion='gini',n_jobs=-1, random_state=42, verbose=1)\n",
    "    RF_phone.fit(t,t_label)\n",
    "    train_df.loc[train_df[train_df.K==k].index,'rf_phone'] = RF_phone.predict_proba(v)[:,1]\n",
    "    test_df_list[k]['rf_phone'] = RF_phone.predict_proba(test_df_list[k][phone_col])[:,1]\n",
    "    \n",
    "for k in tqdm_notebook(range(5)):\n",
    "    t = train_df[train_df.K!=k].reset_index(drop=True)[app_col]\n",
    "    t_label = train_df[train_df.K!=k].reset_index(drop=True).label.values\n",
    "    v = train_df[train_df.K==k].reset_index(drop=True)[app_col]\n",
    "    v_label = train_df[train_df.K==k].reset_index(drop=True).label.values\n",
    "    \n",
    "    RF_app = RandomForestClassifier(n_estimators=10, criterion='gini',n_jobs=-1, random_state=42, verbose=1)\n",
    "    RF_app.fit(t,t_label)\n",
    "    train_df.loc[train_df[train_df.K==k].index,'rf_app'] = RF_app.predict_proba(v)[:,1]\n",
    "    test_df_list[k]['rf_app'] = RF_app.predict_proba(test_df_list[k][app_col])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.roc_auc_score(train_df.label,train_df.rf_user))\n",
    "print(sklearn.metrics.roc_auc_score(train_df.label,train_df.rf_ad))\n",
    "print(sklearn.metrics.roc_auc_score(train_df.label,train_df.rf_phone))\n",
    "print(sklearn.metrics.roc_auc_score(train_df.label,train_df.rf_app))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_feature = [col for col in train_df.columns if col not in drop_cols+cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = cat_cols+dense_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  feature_importance_df = pd.DataFrame()\n",
    "  predicts = np.zeros(len(train_df))\n",
    "  pred = np.zeros(len(test_df_list[0]))\n",
    "  true = np.zeros(len(train_df))\n",
    "\n",
    "\n",
    "  begin = 0\n",
    "  for fold,k in enumerate(range(5)):\n",
    "    \n",
    "    #train = train_df[train_df.K!=k].reset_index(drop=True)[feature]\n",
    "    t_label = train_df[train_df.K!=k].reset_index(drop=True).label\n",
    "    #valid = train_df[train_df.K==k].reset_index(drop=True)[feature]\n",
    "    te_label = train_df[train_df.K==k].reset_index(drop=True).label\n",
    "    \n",
    "\n",
    "  \n",
    "    clf = cbt.CatBoostClassifier(iterations = 150, learning_rate = 0.3, depth =7, one_hot_max_size=5,use_best_model =True,\n",
    "                                 loss_function = 'Logloss', eval_metric= \"AUC\",logging_level='Verbose',task_type='GPU',\n",
    "                               cat_features=cat_cols,)#counter_calc_method='Full'，l2_leaf_reg = 10,)\n",
    "      \n",
    "\n",
    "\n",
    "    clf.fit(train_df[train_df.K!=k].reset_index(drop=True)[feature],t_label.astype('int32'),\n",
    "              eval_set=(train_df[train_df.K==k].reset_index(drop=True)[feature], te_label.astype('int32'))\n",
    "          ,plot=True,verbose=1,cat_features=cat_cols)\n",
    "#     predicts[begin:over] = clf.predict_proba(train_df[train_df.K==k].reset_index(drop=True)[feature])[:,1]\n",
    "\n",
    "#     true[begin:over] = te_label.values\n",
    "    pred += (clf.predict_proba(test_df_list[fold][feature])[:,1])\n",
    "#     begin+=len(train_df[train_df.K==k].reset_index(drop=True)[feature])\n",
    "    gc.collect()\n",
    "  print('--------------------')\n",
    "  \n",
    "  #print(sklearn.metrics.roc_auc_score(true,predicts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame()\n",
    "feature_importance_df[\"importance\"] = clf.feature_importances_\n",
    "feature_importance_df[\"feature\"] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df.sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------模型预测----------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8320243359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((pred-np.min(pred))/(np.max(pred)-np.min(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res['id'] = test_df_list[0]['id'].astype('int32')\n",
    "res['probability'] = pred\n",
    "res.to_csv('5cv_catboost_baseline_target_encoding_.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
